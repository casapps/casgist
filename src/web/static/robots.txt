# CasGists robots.txt
# This file tells web robots which pages to crawl and which to avoid

User-agent: *
Allow: /
Allow: /public
Allow: /gist/*
Allow: /user/*
Allow: /search
Allow: /api/v1/docs

# Disallow private areas
Disallow: /admin
Disallow: /setup
Disallow: /api/v1/auth/
Disallow: /api/v1/admin/
Disallow: /api/v1/user/gists
Disallow: /api/v1/webhooks
Disallow: /api/v1/backup
Disallow: /offline

# Allow specific API documentation
Allow: /api/v1/docs
Allow: /api/v1/health

# Crawl delay (be nice to the server)
Crawl-delay: 1

# Sitemap location
Sitemap: /sitemap.xml